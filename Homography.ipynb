{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5faec206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b555ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"panorama_results\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "def save_file(output_filename,my_image_to_save):\n",
    "    full_path = os.path.join(output_folder, output_filename)\n",
    "    cv2.imwrite(full_path, my_image_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "87a0c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_points(pts):\n",
    "    # Used for later in normalizing ar images\n",
    "    mean = np.mean(pts, axis=0)\n",
    "    cx, cy = mean\n",
    "    pts_centered = pts - mean\n",
    "    avg_dist = np.mean(np.linalg.norm(pts_centered, axis=1))\n",
    "    scale = np.sqrt(2) / avg_dist if avg_dist > 0 else 1.0\n",
    "    #normalizer matrix\n",
    "    T = np.array([\n",
    "        [scale, 0, -scale * cx],\n",
    "        [0, scale, -scale * cy],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    pts_h = np.hstack((pts, np.ones((pts.shape[0], 1))))\n",
    "    pts_norm_h = (T @ pts_h.T).T\n",
    "    pts_norm = pts_norm_h[:, :2] / pts_norm_h[:, 2, np.newaxis]\n",
    "    # new point normalized\n",
    "    return pts_norm, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "35d34bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sift(image_dir,printit,save_it=False):\n",
    "    # same with every part\n",
    "    ret=[]\n",
    "    image_pattern = os.path.join(image_dir, '*.png') \n",
    "    image_files = glob.glob(image_pattern)\n",
    "    for file_path in image_files:\n",
    "        gray = cv2.imread(file_path, cv2.COLOR_BGR2GRAY)\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(filename)\n",
    "        sift = cv2.SIFT_create()\n",
    "        # get key points using method\n",
    "        kp, des = sift.detectAndCompute(gray,None)\n",
    "        ret.append([kp,des,filename])\n",
    "        if(printit):    \n",
    "            # show the images\n",
    "            img=cv2.drawKeypoints(gray,kp,gray)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        if save_it:\n",
    "            img=cv2.drawKeypoints(gray,kp,gray)\n",
    "            save_file(\"sift\"+filename,img)\n",
    "    return ret,image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d7273b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orb(image_dir,printit,save_it=False):\n",
    "    ret=[]\n",
    "\n",
    "    image_pattern = os.path.join(image_dir, '*.png') \n",
    "    image_files = glob.glob(image_pattern)\n",
    "    for file_path in image_files:\n",
    "        gray = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        filename = os.path.basename(file_path)\n",
    "        orb = cv2.ORB_create()\n",
    "        kp = orb.detect(gray,None)\n",
    "        kp, des = orb.compute(gray, kp)\n",
    "        ret.append([kp,des,filename])\n",
    "        if(printit):    \n",
    "            # show the images\n",
    "            img=cv2.drawKeypoints(gray,kp,gray)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "        if save_it:\n",
    "            img=cv2.drawKeypoints(gray,kp,gray)\n",
    "            save_file(\"orb\"+filename,img)\n",
    "    return ret,image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "527d6543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png\n",
      "2.png\n",
      "3.png\n",
      "4.png\n",
      "5.png\n",
      "6.png\n"
     ]
    }
   ],
   "source": [
    "image_dir=\"panorama_dataset\\\\v_bird\"\n",
    "list_sift=sift(image_dir,False,True)\n",
    "list_orb=orb(image_dir,False,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8309ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(keypoints1, descriptors1, keypoints2, descriptors2, norm_type):\n",
    "    # checking key points and histogram to see if they are a match using distance\n",
    "    matcher = cv2.BFMatcher(norm_type)\n",
    "    candidate_matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    best_matches = []\n",
    "    ratio_limit = 0.75\n",
    "    for pair in candidate_matches:\n",
    "        if len(pair) == 2: \n",
    "            match1, match2 = pair\n",
    "            # if got a good match add it\n",
    "            if match1.distance < ratio_limit * match2.distance:\n",
    "                best_matches.append(match1)\n",
    "                \n",
    "    source_points = np.array([ keypoints1[match.queryIdx].pt for match in best_matches ], dtype=np.float32).reshape(-1, 2)\n",
    "    destination_points = np.array([ keypoints2[match.trainIdx].pt for match in best_matches ], dtype=np.float32).reshape(-1, 2)\n",
    "    \n",
    "    # return matches source and destination\n",
    "    return best_matches, source_points, destination_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f325e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(lists,norm=cv2.NORM_L2):\n",
    "    # from images get the matches\n",
    "    image_matches=[]\n",
    "    for i in range(len(lists)):\n",
    "        kp1_list=lists[i][0]\n",
    "        kp1_hist=lists[i][1]\n",
    "        # use the keypoint list and hists\n",
    "        for j in range(i+1,len(lists)):\n",
    "            kp2_list=lists[j][0]\n",
    "            kp2_hist=lists[j][1]\n",
    "            # give the kp and hist go get the matching points\n",
    "            good_matches,src_pts, dst_pts=find_matches(kp1_list,kp1_hist,kp2_list,kp2_hist,norm)\n",
    "            # add all valid matches\n",
    "            image_matches.append([i,j,good_matches,src_pts, dst_pts])\n",
    "    return image_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9184b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.png\n",
      "2.png\n",
      "3.png\n",
      "4.png\n",
      "5.png\n",
      "6.png\n"
     ]
    }
   ],
   "source": [
    "list_sift, image_files = sift(image_dir, False)\n",
    "\n",
    "image_matches_sift = matcher(list_sift, cv2.NORM_L2)\n",
    "# after getting the match points showing them\n",
    "\n",
    "for i, j, good_matches, src_pts, dst_pts in image_matches_sift:\n",
    "    \n",
    "    img1 = cv2.imread(image_files[i])\n",
    "    img2 = cv2.imread(image_files[j])\n",
    "\n",
    "    kp1 = list_sift[i][0]\n",
    "    kp2 = list_sift[j][0]\n",
    "\n",
    "    img_with_matches = cv2.drawMatches(\n",
    "        img1, kp1,          # type: ignore\n",
    "        img2, kp2,          # type: ignore\n",
    "        good_matches,      \n",
    "        None,              # type: ignore\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )  # type: ignore\n",
    "\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img_with_matches, cv2.COLOR_BGR2RGB)\n",
    "    save_file(image_files[i],img_rgb)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "871ec45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_orb, image_files = orb(image_dir, False)\n",
    "\n",
    "image_matches_orb = matcher(list_orb, cv2.NORM_HAMMING)\n",
    "# same with orb\n",
    "\n",
    "for i, j, good_matches, src_pts, dst_pts in image_matches_orb:\n",
    "    \n",
    "    img1 = cv2.imread(image_files[i])\n",
    "    img2 = cv2.imread(image_files[j])\n",
    "\n",
    "    kp1 = list_orb[i][0]\n",
    "    kp2 = list_orb[j][0]\n",
    "\n",
    "    img_with_matches = cv2.drawMatches(\n",
    "        img1, kp1,          # type: ignore\n",
    "        img2, kp2,          # type: ignore\n",
    "        good_matches,      \n",
    "        None,              # type: ignore\n",
    "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )  # type: ignore\n",
    "\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img_with_matches, cv2.COLOR_BGR2RGB)\n",
    "    save_file(image_files[i],img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08283810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_transform(src_pts, dst_pts):\n",
    "    src_norm, T_src = normalize_points(src_pts)\n",
    "    dst_norm, T_dst = normalize_points(dst_pts)\n",
    "    # create the A matrix for solition\n",
    "    A = []\n",
    "    for (x, y), (xp, yp) in zip(src_norm, dst_norm):\n",
    "        row1 = [0, 0, 0, -x, -y, -1, yp * x, yp * y, yp]\n",
    "        row2 = [x, y, 1, 0, 0, 0, -xp * x, -xp * y, -xp]\n",
    "        A.append(row1)\n",
    "        A.append(row2)\n",
    "\n",
    "    A = np.array(A, dtype=np.float32)\n",
    "    # get the H array solving A\n",
    "    U, S, Vh = np.linalg.svd(A)\n",
    "    h = Vh[-1]\n",
    "    H_norm = h.reshape(3, 3)\n",
    "\n",
    "    H = np.linalg.inv(T_dst) @ H_norm @ T_src\n",
    "    # normalize the last var\n",
    "    H = H / H[2, 2]\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23e44361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RANSAC(source_points, destination_points, max_iterations=1000, distance_threshold=5.0):\n",
    "    # get the inliers of matchrs using RANCSAC\n",
    "    best_inlier_indices = []\n",
    "    max_inlier_count = 0\n",
    "    num_points = source_points.shape[0]\n",
    "    \n",
    "    if num_points < 4:\n",
    "        # Not enough points, fail fast\n",
    "        return None, []\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        # get 4 random points try to get a H matrix and test it\n",
    "        # if gives good results keep it\n",
    "        random_indices = random.sample(range(num_points), 4)\n",
    "        source_sample = source_points[random_indices]\n",
    "        destination_sample = destination_points[random_indices]\n",
    "        \n",
    "        # find solition\n",
    "        try:\n",
    "            # Assuming direct_transform is defined elsewhere\n",
    "            H_candidate = direct_transform(source_sample, destination_sample) \n",
    "        except np.linalg.LinAlgError:\n",
    "            continue\n",
    "\n",
    "        current_inlier_indices = []\n",
    "        for point_index in range(num_points):\n",
    "            source_point_homogeneous = np.array([source_points[point_index, 0], source_points[point_index, 1], 1.0])\n",
    "            destination_point_actual = destination_points[point_index]\n",
    "            \n",
    "            predicted_point_homogeneous = H_candidate @ source_point_homogeneous\n",
    "            \n",
    "            if predicted_point_homogeneous[2] == 0: \n",
    "                continue\n",
    "                \n",
    "            predicted_point = (predicted_point_homogeneous[0] / predicted_point_homogeneous[2], predicted_point_homogeneous[1] / predicted_point_homogeneous[2])\n",
    "            \n",
    "            error_distance = np.linalg.norm(np.array(predicted_point) - destination_point_actual)\n",
    "            # test if it gets close the real point\n",
    "            if error_distance < distance_threshold:\n",
    "                current_inlier_indices.append(point_index)\n",
    "\n",
    "        if len(current_inlier_indices) > max_inlier_count:\n",
    "            max_inlier_count = len(current_inlier_indices)\n",
    "            best_inlier_indices = current_inlier_indices\n",
    "\n",
    "    if max_inlier_count < 4:\n",
    "        # Failed to find a good model\n",
    "        return None, [] \n",
    "    \n",
    "    # using best inliers get the H matrix\n",
    "    # Assuming direct_transform is defined elsewhere\n",
    "    final_homography = direct_transform(source_points[best_inlier_indices], destination_points[best_inlier_indices])\n",
    "    return final_homography, best_inlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc722306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9646017699115044\n",
      "0.9459459459459459\n",
      "0.9714285714285714\n",
      "0.8205128205128205\n",
      "0.625\n",
      "0.9854014598540146\n",
      "0.8679245283018868\n",
      "0.8571428571428571\n",
      "0.4444444444444444\n",
      "0.9830508474576272\n",
      "0.7777777777777778\n",
      "0.9333333333333333\n",
      "0.968503937007874\n",
      "0.7272727272727273\n",
      "0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "for i, j, good_matches, src_pts, dst_pts in image_matches_orb:\n",
    "    result = Ransac(src_pts, dst_pts)\n",
    "    H_best, best = result\n",
    "    if H_best is None:\n",
    "        print(\"Bad points\")\n",
    "        continue\n",
    "    else:\n",
    "        print(len(best)/src_pts.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "85b986a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, good_matches, src_pts, dst_pts in image_matches_orb:\n",
    "    \n",
    "    result = Ransac(src_pts, dst_pts)\n",
    "    H_best, best_inlier_indices = result\n",
    "    \n",
    "    img1 = cv2.imread(image_files[i])\n",
    "    img2 = cv2.imread(image_files[j])\n",
    "    kp1 = list_orb[i][0]\n",
    "    kp2 = list_orb[j][0]\n",
    "\n",
    "    if H_best is None:\n",
    "        print(f\"RANSAC FAILED for pair {i},{j}. Not enough inliers.\")\n",
    "        \n",
    "    else:\n",
    "        inlier_matches = []\n",
    "        outlier_matches = []\n",
    "        # find good and bad indices kps\n",
    "        all_indices = set(range(len(good_matches)))\n",
    "        inlier_indices = set(best_inlier_indices)\n",
    "        outlier_indices = all_indices - inlier_indices\n",
    "        for k in inlier_indices:\n",
    "            inlier_matches.append(good_matches[k])\n",
    "        for k in outlier_indices:\n",
    "            outlier_matches.append(good_matches[k])\n",
    "        # draw lines on inlers and outliers\n",
    "        img_inliers = cv2.drawMatches(\n",
    "            img1, kp1, img2, kp2, inlier_matches, None,\n",
    "            matchColor=(0, 255, 0), \n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "        )\n",
    "\n",
    "        img_viz = cv2.drawMatches(\n",
    "            img1, kp1, img2, kp2, outlier_matches, img_inliers,\n",
    "            matchColor=(255, 0, 0),  \n",
    "            flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS | cv2.DrawMatchesFlags_DRAW_OVER_OUTIMG\n",
    "        )\n",
    "    file_i_name = os.path.basename(image_files[i])\n",
    "    file_j_name = os.path.basename(image_files[j])\n",
    "    \n",
    "    # Create a clean, valid filename\n",
    "    save_filename = f\"matched_{i}_{file_i_name}_to_{j}_{file_j_name}\"\n",
    "    save_file(save_filename, img_viz)\n",
    "    img_rgb = cv2.cvtColor(img_viz, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_panorama(imageset, image_list=None, shiftlist=None, show_overlap=False):\n",
    "    if image_list is None:\n",
    "        print(imageset)\n",
    "        list_sift, image_files = sift(imageset, False)\n",
    "    else:\n",
    "        image_files = image_list\n",
    "        list_sift = shiftlist\n",
    "    \n",
    "    image_matches_sift = matcher(list_sift, cv2.NORM_L2) \n",
    "    \n",
    "    \n",
    "    img_ref = cv2.imread(image_files[0])\n",
    "    h_ref, w_ref = img_ref.shape[:2]\n",
    "\n",
    "    \n",
    "    homographies = {} \n",
    "    \n",
    "    # start wtih bounderis\n",
    "    ref_corners = np.float32([[0, 0], [w_ref, 0], [w_ref, h_ref], [0, h_ref]]).reshape(-1, 1, 2)\n",
    "    \n",
    "    \n",
    "    all_corners = [ref_corners]\n",
    "\n",
    "    for i, j, good_matches, src_pts, dst_pts in image_matches_sift:\n",
    "        \n",
    "        if i != 0:\n",
    "            continue\n",
    "        \n",
    "        if len(good_matches) < 4:\n",
    "            print(f\"Skipping image {j} (Only {len(good_matches)} matches)\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        H_j_to_i, best_inliers = Ransac(dst_pts, src_pts, iterations=2000, thresh=5.0)\n",
    "        \n",
    "        \n",
    "        homographies[j] = H_j_to_i\n",
    "        # get H best\n",
    "        \n",
    "        img_target = cv2.imread(image_files[j])\n",
    "        h_t, w_t = img_target.shape[:2]\n",
    "        \n",
    "        # get the corner sizes\n",
    "        target_corners = np.float32([[0, 0], [w_t, 0], [w_t, h_t], [0, h_t]]).reshape(-1, 1, 2)\n",
    "        \n",
    "        \n",
    "        warped_corners = cv2.perspectiveTransform(target_corners, H_j_to_i)\n",
    "        all_corners.append(warped_corners)\n",
    "\n",
    "    \n",
    "    all_pts = np.concatenate(all_corners, axis=0)\n",
    "    \n",
    "    # find bounding box\n",
    "    x_min, y_min = np.int32(all_pts.min(axis=0).ravel() - 0.5)\n",
    "    x_max, y_max = np.int32(all_pts.max(axis=0).ravel() + 0.5)\n",
    "    \n",
    "    \n",
    "    translation_dist = [-x_min, -y_min]\n",
    "    \n",
    "    #sift the canvas to desiered\n",
    "    canvas_w = x_max - x_min\n",
    "    canvas_h = y_max - y_min\n",
    "\n",
    "    print(f\"Calculated Panorama Size: {canvas_w}x{canvas_h}\")\n",
    "    \n",
    "    \n",
    "    H_translation = np.array([\n",
    "        [1, 0, translation_dist[0]],\n",
    "        [0, 1, translation_dist[1]],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    \n",
    "    \n",
    "    pano_image = np.zeros((canvas_h, canvas_w, 3), dtype=np.uint8)\n",
    "    \n",
    "    \n",
    "    pano_image = cv2.warpPerspective(img_ref, H_translation, (canvas_w, canvas_h))\n",
    "\n",
    "   \n",
    "    for j, H_raw in homographies.items():\n",
    "        #warp the image and put it onto the overlap using weighed\n",
    "        H_final = H_translation @ H_raw\n",
    "        \n",
    "        img_target = cv2.imread(image_files[j])\n",
    "        warped_target = cv2.warpPerspective(img_target, H_final, (canvas_w, canvas_h))\n",
    "        \n",
    "        mask_warped = cv2.cvtColor(warped_target, cv2.COLOR_BGR2GRAY) > 0\n",
    "        mask_ref = cv2.cvtColor(pano_image, cv2.COLOR_BGR2GRAY) > 0\n",
    "        \n",
    "        overlap_mask = mask_ref & mask_warped\n",
    "        non_overlap_mask = mask_warped & ~mask_ref\n",
    "        \n",
    "        if show_overlap:\n",
    "            overlap_visual = np.zeros(pano_image.shape, dtype=np.uint8)\n",
    "            overlap_visual[overlap_mask] = pano_image[overlap_mask]\n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.imshow(cv2.cvtColor(overlap_visual, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Overlap Region: Image {j}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        pano_image[non_overlap_mask] = warped_target[non_overlap_mask]\n",
    "        \n",
    "        pano_image[overlap_mask] = cv2.addWeighted(\n",
    "            pano_image[overlap_mask], 0.5, \n",
    "            warped_target[overlap_mask], 0.5, 0\n",
    "        )\n",
    "\n",
    "    print(\"All images merged.\")\n",
    "    name=imageset[17:]\n",
    "    save_file(\"panorama\"+name+\".png\",pano_image)\n",
    "\n",
    "    return pano_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed722a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dirs=[\"panorama_dataset\\\\v_bird\",\"panorama_dataset\\\\v_boat\",\"panorama_dataset\\\\v_circus\",\"panorama_dataset\\\\v_graffiti\",\"panorama_dataset\\\\v_soldiers\",\"panorama_dataset\\\\v_weapons\"]\n",
    "\n",
    "for images in image_dirs:\n",
    "    panorama=get_panorama(images,show_overlap=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4c7b3",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8fb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_aspect(image, ref_h, ref_w):\n",
    "    ref_aspect = ref_w / ref_h\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    img_aspect = img_w / img_h\n",
    "    \n",
    "    if img_aspect > ref_aspect:\n",
    "        # image is wider than reference: Crop sides\n",
    "        new_w = int(ref_aspect * img_h)\n",
    "        x_start = (img_w - new_w) // 2\n",
    "        return image[:, x_start:x_start + new_w]\n",
    "    else:\n",
    "        # image is taller than reference: Crop top/bottom\n",
    "        new_h = int(img_w / ref_aspect)\n",
    "        y_start = (img_h - new_h) // 2\n",
    "        return image[y_start:y_start + new_h, :]\n",
    "\n",
    "\n",
    "print(\"Setting up AR pipeline...\")\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "img_ref = cv2.imread(\"ar_dataset/cv_cover.jpg\")\n",
    "if img_ref is None:\n",
    "    print(\"Error: Could not read ar_dataset/cv_cover.jpg\")\n",
    "else:\n",
    "    gray_ref = cv2.cvtColor(img_ref, cv2.COLOR_BGR2GRAY)\n",
    "    kp_ref, des_ref = sift.detectAndCompute(gray_ref, None)\n",
    "    h_ref, w_ref = img_ref.shape[:2]\n",
    "    print(f\"Reference image loaded: {w_ref}x{h_ref}\")\n",
    "\n",
    "# open the TARGET video \n",
    "cap_target = cv2.VideoCapture(\"ar_dataset/book.mov\")\n",
    "cap_source = cv2.VideoCapture(\"ar_dataset/ar_source.mov\")\n",
    "\n",
    "w_target = int(cap_target.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h_target = int(cap_target.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap_target.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "output_filename = \"ar_dynamic_result.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_filename, fourcc, fps, (w_target, h_target))\n",
    "\n",
    "\n",
    "frame_count = 0\n",
    "while cap_target.isOpened():\n",
    "    \n",
    "    ret_t, target_frame = cap_target.read()\n",
    "    if not ret_t:\n",
    "        print(\"End of target video.\")\n",
    "        break\n",
    "    # get the frame\n",
    "    ret_s, source_frame = cap_source.read()\n",
    "    if not ret_s:\n",
    "        cap_source.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        ret_s, source_frame = cap_source.read()\n",
    "        if not ret_s:\n",
    "            print(\"Error reading source video, even after looping.\")\n",
    "            break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(target_frame, cv2.COLOR_BGR2GRAY)\n",
    "    kp_frame, des_frame = sift.detectAndCompute(gray_frame, None)\n",
    "    \n",
    "    good_matches, src_pts, dst_pts = match_features(kp_ref, des_ref, kp_frame, des_frame, cv2.NORM_L2)\n",
    "    # get the book match square\n",
    "    H_ref_to_target = None\n",
    "    if len(good_matches) > 4:\n",
    "        H_ref_to_target, _ = Ransac(src_pts, dst_pts, iterations=1000, thresh=5.0)\n",
    "\n",
    "    if H_ref_to_target is not None:\n",
    "        # get the part of the book and warp the video to that book cover\n",
    "        cropped_source = crop_to_aspect(source_frame, h_ref, w_ref)\n",
    "        \n",
    "        resized_source = cv2.resize(cropped_source, (w_ref, h_ref))\n",
    "\n",
    "        warped_source = cv2.warpPerspective(resized_source, H_ref_to_target, (w_target, h_target))\n",
    "        \n",
    "        mask = cv2.cvtColor(warped_source, cv2.COLOR_BGR2GRAY) > 0\n",
    "        # than just paste the video frame \n",
    "        final_frame = target_frame.copy()\n",
    "        \n",
    "        final_frame[mask] = warped_source[mask]\n",
    "\n",
    "    else:\n",
    "        final_frame = target_frame\n",
    "        print(f\"Frame {frame_count}: Homography failed (matches={len(good_matches)})\")\n",
    "\n",
    "    out.write(final_frame)\n",
    "    cv2.imshow(\"AR Result\", final_frame)\n",
    "    # get to the next frame and go on until end\n",
    "    frame_count += 1\n",
    "    print(frame_count)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap_target.release()\n",
    "cap_source.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"AR processing complete. Saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
